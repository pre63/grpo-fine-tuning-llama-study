{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from scipy.stats import chi2_contingency, f_oneway\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current NLTK Data Path: ['/Users/dach/nltk_data', '/Users/dach/dev/grpo-fine-tuning-llama-study/.venv/nltk_data', '/Users/dach/dev/grpo-fine-tuning-llama-study/.venv/share/nltk_data', '/Users/dach/dev/grpo-fine-tuning-llama-study/.venv/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data']\n",
      "'punkt' is already downloaded.\n",
      "'punkt_tab' not found. Downloading now...\n",
      "Successfully downloaded 'punkt_tab'.\n",
      "'stopwords' is already downloaded.\n",
      "Tokenization test successful: ['This', 'is', 'a', 'test', 'sentence', '.']\n",
      "Stopwords test successful: ['during', 's', 'over', 'shouldn', 'll']\n",
      "Sample tokens: [['what', 'is', 'the', 'capital'], ['solve', 'this', 'equation']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /Users/dach/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Function to debug and setup NLTK\n",
    "def setup_nltk():\n",
    "  # Define required NLTK datasets\n",
    "  required_datasets = [\"punkt\", \"punkt_tab\", \"stopwords\"]\n",
    "\n",
    "  # Print current NLTK data path\n",
    "  print(\"Current NLTK Data Path:\", nltk.data.path)\n",
    "\n",
    "  # Check if datasets are already downloaded\n",
    "  for dataset in required_datasets:\n",
    "    try:\n",
    "      nltk.data.find(f\"tokenizers/{dataset}\" if dataset == \"punkt\" else f\"corpora/{dataset}\")\n",
    "      print(f\"'{dataset}' is already downloaded.\")\n",
    "    except LookupError:\n",
    "      print(f\"'{dataset}' not found. Downloading now...\")\n",
    "      try:\n",
    "        nltk.download(dataset, quiet=False)\n",
    "        print(f\"Successfully downloaded '{dataset}'.\")\n",
    "      except Exception as e:\n",
    "        print(f\"Failed to download '{dataset}': {e}\")\n",
    "\n",
    "  # Verify download by testing\n",
    "  try:\n",
    "    sample_text = \"This is a test sentence.\"\n",
    "    tokens = word_tokenize(sample_text)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    print(\"Tokenization test successful:\", tokens)\n",
    "    print(\"Stopwords test successful:\", list(stop_words)[:5])\n",
    "  except LookupError as e:\n",
    "    print(\"NLTK setup failed:\", e)\n",
    "    print(\"Manually set NLTK data path as a fallback...\")\n",
    "    custom_path = os.path.expanduser(\"~/nltk_data\")  # Default user path\n",
    "    nltk.data.path.append(custom_path)\n",
    "    print(\"Updated NLTK Data Path:\", nltk.data.path)\n",
    "    # Retry download\n",
    "    for dataset in required_datasets:\n",
    "      nltk.download(dataset, download_dir=custom_path, quiet=False)\n",
    "\n",
    "\n",
    "# Run the setup\n",
    "setup_nltk()\n",
    "\n",
    "# Example usage in your script\n",
    "text_series = pd.Series([\"What is the capital?\", \"Solve this equation.\"])\n",
    "tokens = text_series.apply(lambda x: word_tokenize(re.sub(r\"[^\\w\\s]\", \"\", str(x).lower())) if pd.notnull(x) else [])\n",
    "print(\"Sample tokens:\", tokens.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "dataset_name = \"cais/hle\"\n",
    "hf_split = \"test\"\n",
    "domain_col = \"category\"  # Adjust if needed\n",
    "test_size = 0.3  # 30% for evaluation\n",
    "min_count_per_domain = 50  # Minimum samples per domain for sufficiency check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(dataset_name, split=hf_split)\n",
    "df = pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratum Counts:\n",
      " strat_key\n",
      "Math_exactMatch_False                             964\n",
      "Physics_exactMatch_False                          188\n",
      "Computer Science/AI_exactMatch_False              171\n",
      "Biology/Medicine_multipleChoice_False             159\n",
      "Other_exactMatch_False                            145\n",
      "Humanities/Social Science_exactMatch_False        123\n",
      "Math_multipleChoice_False                          97\n",
      "Humanities/Social Science_multipleChoice_False     87\n",
      "Biology/Medicine_exactMatch_False                  85\n",
      "Chemistry_exactMatch_False                         81\n",
      "Computer Science/AI_multipleChoice_False           72\n",
      "Other_multipleChoice_False                         55\n",
      "Engineering_exactMatch_False                       53\n",
      "Chemistry_exactMatch_True                          45\n",
      "Engineering_exactMatch_True                        42\n",
      "Other_exactMatch_True                              42\n",
      "Physics_multipleChoice_False                       38\n",
      "Biology/Medicine_exactMatch_True                   38\n",
      "Math_exactMatch_True                               37\n",
      "Chemistry_multipleChoice_False                     27\n",
      "Engineering_multipleChoice_False                   25\n",
      "Biology/Medicine_multipleChoice_True               21\n",
      "Humanities/Social Science_exactMatch_True          17\n",
      "Chemistry_multipleChoice_True                      17\n",
      "Other_multipleChoice_True                          16\n",
      "Physics_exactMatch_True                            11\n",
      "Computer Science/AI_exactMatch_True                10\n",
      "Engineering_multipleChoice_True                    10\n",
      "Humanities/Social Science_multipleChoice_True       8\n",
      "Math_multipleChoice_True                            8\n",
      "Computer Science/AI_multipleChoice_True             5\n",
      "Physics_multipleChoice_True                         3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== Category-wise Analysis ===\n",
      "\n",
      "Category: Other\n",
      "\n",
      "Question Text Metrics:\n",
      "                                                                                          0\n",
      "Total Words                                                                           20302\n",
      "Vocabulary Size                                                                        3755\n",
      "TTR                                                                       0.373446046742914\n",
      "Lexical Density                                                          0.4952714018323318\n",
      "Avg Words per Entry                                                       78.68992248062015\n",
      "Top 5 Words          [('answer', 133), ('b', 123), ('c', 123), ('e', 108), ('choices', 72)]\n",
      "\n",
      "Rationale Text Metrics:\n",
      "                                                                                              0\n",
      "Total Words                                                                               30990\n",
      "Vocabulary Size                                                                            5086\n",
      "TTR                                                                         0.33045286206224417\n",
      "Lexical Density                                                              0.4966440787350758\n",
      "Avg Words per Entry                                                          120.11627906976744\n",
      "Top 5 Words          [('black', 126), ('one', 111), ('peg', 110), ('white', 103), ('king', 98)]\n",
      "\n",
      "Image Proportion: 22.48%\n",
      "\n",
      "Answer Type Distribution:\n",
      "exactMatch        0.7248062015503876\n",
      "multipleChoice    0.2751937984496124\n",
      "Image Chi2: 124.50, P-value: 0.2780\n",
      "\n",
      "Answer Type Split Contingency Table:\n",
      "                    Full     Train      Test\n",
      "answer_type                                 \n",
      "exactMatch      0.724806  0.726257  0.721519\n",
      "multipleChoice  0.275194  0.273743  0.278481\n",
      "Answer Type Chi2: 0.02, P-value: 0.9924\n",
      "\n",
      "Category: Humanities/Social Science\n",
      "\n",
      "Question Text Metrics:\n",
      "                                                                                      0\n",
      "Total Words                                                                       31938\n",
      "Vocabulary Size                                                                    5779\n",
      "TTR                                                                  0.3244988488966253\n",
      "Lexical Density                                                      0.5576116225186298\n",
      "Avg Words per Entry                                                   135.9063829787234\n",
      "Top 5 Words          [('b', 148), ('answer', 146), ('c', 128), ('e', 116), ('one', 99)]\n",
      "\n",
      "Rationale Text Metrics:\n",
      "                                                                                                     0\n",
      "Total Words                                                                                      42711\n",
      "Vocabulary Size                                                                                   7309\n",
      "TTR                                                                                  0.315437400198524\n",
      "Lexical Density                                                                     0.5425066142211609\n",
      "Avg Words per Entry                                                                 181.74893617021277\n",
      "Top 5 Words          [('answer', 152), ('would', 146), ('option', 119), ('one', 106), ('correct', 83)]\n",
      "\n",
      "Image Proportion: 10.64%\n",
      "\n",
      "Answer Type Distribution:\n",
      "exactMatch         0.5957446808510638\n",
      "multipleChoice    0.40425531914893614\n",
      "Image Chi2: 52.47, P-value: 0.3783\n",
      "\n",
      "Answer Type Split Contingency Table:\n",
      "                    Full     Train  Test\n",
      "answer_type                             \n",
      "exactMatch      0.595745  0.593939   0.6\n",
      "multipleChoice  0.404255  0.406061   0.4\n",
      "Answer Type Chi2: 0.02, P-value: 0.9906\n",
      "\n",
      "Category: Math\n",
      "\n",
      "Question Text Metrics:\n",
      "                                                                                       0\n",
      "Total Words                                                                       100551\n",
      "Vocabulary Size                                                                     6943\n",
      "TTR                                                                  0.13699954616310503\n",
      "Lexical Density                                                       0.5040128889817108\n",
      "Avg Words per Entry                                                    90.91410488245931\n",
      "Top 5 Words          [('x', 679), ('let', 614), ('n', 609), ('b', 470), ('number', 442)]\n",
      "\n",
      "Rationale Text Metrics:\n",
      "                                                                                         0\n",
      "Total Words                                                                         266313\n",
      "Vocabulary Size                                                                      10591\n",
      "TTR                                                                    0.08163123737879792\n",
      "Lexical Density                                                         0.4871786206456313\n",
      "Avg Words per Entry                                                     240.78933092224233\n",
      "Top 5 Words          [('x', 11894), ('z', 5925), ('w', 2269), ('n', 1154), ('right', 803)]\n",
      "\n",
      "Image Proportion: 4.07%\n",
      "\n",
      "Answer Type Distribution:\n",
      "exactMatch        0.9050632911392406\n",
      "multipleChoice    0.0949367088607595\n",
      "Image Chi2: 95.12, P-value: 0.2348\n",
      "\n",
      "Answer Type Split Contingency Table:\n",
      "                    Full     Train      Test\n",
      "answer_type                                 \n",
      "exactMatch      0.905063  0.904393  0.906627\n",
      "multipleChoice  0.094937  0.095607  0.093373\n",
      "Answer Type Chi2: 0.03, P-value: 0.9832\n",
      "\n",
      "Category: Physics\n",
      "\n",
      "Question Text Metrics:\n",
      "                                                                                                  0\n",
      "Total Words                                                                                   27563\n",
      "Vocabulary Size                                                                                3672\n",
      "TTR                                                                             0.25051166598444535\n",
      "Lexical Density                                                                  0.5317998766462286\n",
      "Avg Words per Entry                                                              114.84583333333333\n",
      "Top 5 Words          [('nm', 412), ('wavelength', 405), ('intensity', 402), ('b', 126), ('c', 116)]\n",
      "\n",
      "Rationale Text Metrics:\n",
      "                                                                                              0\n",
      "Total Words                                                                               50069\n",
      "Vocabulary Size                                                                            5082\n",
      "TTR                                                                         0.20709890378581033\n",
      "Lexical Density                                                              0.4901036569534043\n",
      "Avg Words per Entry                                                          208.62083333333334\n",
      "Top 5 Words          [('times', 486), ('r', 225), ('right', 215), ('left', 194), ('cdot', 190)]\n",
      "\n",
      "Image Proportion: 5.83%\n",
      "\n",
      "Answer Type Distribution:\n",
      "exactMatch         0.8291666666666667\n",
      "multipleChoice    0.17083333333333334\n",
      "Image Chi2: 29.72, P-value: 0.3769\n",
      "\n",
      "Answer Type Split Contingency Table:\n",
      "                    Full     Train      Test\n",
      "answer_type                                 \n",
      "exactMatch      0.829167  0.828402  0.830986\n",
      "multipleChoice  0.170833  0.171598  0.169014\n",
      "Answer Type Chi2: 0.01, P-value: 0.9970\n",
      "\n",
      "Category: Computer Science/AI\n",
      "\n",
      "Question Text Metrics:\n",
      "                                                                                       0\n",
      "Total Words                                                                        50492\n",
      "Vocabulary Size                                                                     5466\n",
      "TTR                                                                  0.21359071548591302\n",
      "Lexical Density                                                       0.5068327655866276\n",
      "Avg Words per Entry                                                   195.70542635658916\n",
      "Top 5 Words          [('x', 258), ('c', 231), ('loss', 228), ('b', 211), ('epoch', 204)]\n",
      "\n",
      "Rationale Text Metrics:\n",
      "                                                                                     0\n",
      "Total Words                                                                      59297\n",
      "Vocabulary Size                                                                   5886\n",
      "TTR                                                                0.19714630225080385\n",
      "Lexical Density                                                     0.5034993338617468\n",
      "Avg Words per Entry                                                 229.83333333333334\n",
      "Top 5 Words          [('x', 325), ('n', 312), ('b', 193), ('c', 150), ('number', 148)]\n",
      "\n",
      "Image Proportion: 5.81%\n",
      "\n",
      "Answer Type Distribution:\n",
      "exactMatch         0.7015503875968992\n",
      "multipleChoice    0.29844961240310075\n",
      "Image Chi2: 26.36, P-value: 0.5531\n",
      "\n",
      "Answer Type Split Contingency Table:\n",
      "                   Full     Train      Test\n",
      "answer_type                                \n",
      "exactMatch      0.70155  0.701657  0.701299\n",
      "multipleChoice  0.29845  0.298343  0.298701\n",
      "Answer Type Chi2: 0.00, P-value: 1.0000\n",
      "\n",
      "Category: Biology/Medicine\n",
      "\n",
      "Question Text Metrics:\n",
      "                                                                                           0\n",
      "Total Words                                                                            45107\n",
      "Vocabulary Size                                                                         6372\n",
      "TTR                                                                       0.2528170131725123\n",
      "Lexical Density                                                           0.5587602811093622\n",
      "Avg Words per Entry                                                       148.86798679867988\n",
      "Top 5 Words          [('b', 299), ('c', 263), ('answer', 228), ('e', 209), ('choices', 192)]\n",
      "\n",
      "Rationale Text Metrics:\n",
      "                                                                                            0\n",
      "Total Words                                                                             39855\n",
      "Vocabulary Size                                                                          6097\n",
      "TTR                                                                        0.2837925898342953\n",
      "Lexical Density                                                            0.5390540710074019\n",
      "Avg Words per Entry                                                        131.53465346534654\n",
      "Top 5 Words          [('answer', 180), ('would', 103), ('b', 98), ('correct', 97), ('c', 95)]\n",
      "\n",
      "Image Proportion: 19.47%\n",
      "\n",
      "Answer Type Distribution:\n",
      "multipleChoice      0.594059405940594\n",
      "exactMatch        0.40594059405940597\n",
      "Image Chi2: 124.58, P-value: 0.3213\n",
      "\n",
      "Answer Type Split Contingency Table:\n",
      "                    Full    Train      Test\n",
      "answer_type                                \n",
      "multipleChoice  0.594059  0.59434  0.593407\n",
      "exactMatch      0.405941  0.40566  0.406593\n",
      "Answer Type Chi2: 0.00, P-value: 0.9997\n",
      "\n",
      "Category: Chemistry\n",
      "\n",
      "Question Text Metrics:\n",
      "                                                                                                  0\n",
      "Total Words                                                                                   14978\n",
      "Vocabulary Size                                                                                2271\n",
      "TTR                                                                             0.29725130890052354\n",
      "Lexical Density                                                                  0.5100814527974362\n",
      "Avg Words per Entry                                                               88.10588235294118\n",
      "Top 5 Words          [('reaction', 113), ('c', 104), ('b', 104), ('product', 79), ('compound', 74)]\n",
      "\n",
      "Rationale Text Metrics:\n",
      "                                                                                                  0\n",
      "Total Words                                                                                   19445\n",
      "Vocabulary Size                                                                                2959\n",
      "TTR                                                                             0.29328972147883836\n",
      "Lexical Density                                                                  0.5188480329133454\n",
      "Avg Words per Entry                                                              114.38235294117646\n",
      "Top 5 Words          [('times', 191), ('reaction', 76), ('group', 75), ('rangle', 69), ('two', 67)]\n",
      "\n",
      "Image Proportion: 36.47%\n",
      "\n",
      "Answer Type Distribution:\n",
      "exactMatch         0.7411764705882353\n",
      "multipleChoice    0.25882352941176473\n",
      "Image Chi2: 126.17, P-value: 0.2866\n",
      "\n",
      "Answer Type Split Contingency Table:\n",
      "                    Full     Train      Test\n",
      "answer_type                                 \n",
      "exactMatch      0.741176  0.739496  0.745098\n",
      "multipleChoice  0.258824  0.260504  0.254902\n",
      "Answer Type Chi2: 0.01, P-value: 0.9927\n",
      "\n",
      "Category: Engineering\n",
      "\n",
      "Question Text Metrics:\n",
      "                                                                                        0\n",
      "Total Words                                                                         21402\n",
      "Vocabulary Size                                                                      2721\n",
      "TTR                                                                    0.2524821378862392\n",
      "Lexical Density                                                        0.5035510699934586\n",
      "Avg Words per Entry                                                    164.63076923076923\n",
      "Top 5 Words          [('power', 172), ('system', 131), ('r', 96), ('let', 92), ('b', 86)]\n",
      "\n",
      "Rationale Text Metrics:\n",
      "                                                                                                      0\n",
      "Total Words                                                                                       30089\n",
      "Vocabulary Size                                                                                    2918\n",
      "TTR                                                                                 0.19607579626394303\n",
      "Lexical Density                                                                      0.4945993552461032\n",
      "Avg Words per Entry                                                                  231.45384615384614\n",
      "Top 5 Words          [('times', 660), ('power', 233), ('cdot', 188), ('approx', 180), ('voltage', 131)]\n",
      "\n",
      "Image Proportion: 40.00%\n",
      "\n",
      "Answer Type Distribution:\n",
      "exactMatch        0.7307692307692307\n",
      "multipleChoice    0.2692307692307692\n",
      "Image Chi2: 106.46, P-value: 0.3105\n",
      "\n",
      "Answer Type Split Contingency Table:\n",
      "                    Full     Train   Test\n",
      "answer_type                              \n",
      "exactMatch      0.730769  0.733333  0.725\n",
      "multipleChoice  0.269231  0.266667  0.275\n",
      "Answer Type Chi2: 0.02, P-value: 0.9881\n",
      "\n",
      "=== Image Cross-Category Analysis ===\n",
      "                           Num Image Questions Q_Total Words Q_Vocabulary Size               Q_TTR    Q_Lexical Density Q_Avg Words per Entry                                                                       Q_Top 5 Words R_Total Words R_Vocabulary Size                R_TTR    R_Lexical Density R_Avg Words per Entry                                                                           R_Top 5 Words\n",
      "Category                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
      "Other                                       58          2224               594                 0.5   0.5341726618705036      38.3448275862069                 [('answer', 30), ('move', 21), ('b', 20), ('plate', 20), ('e', 18)]          4436              1157   0.4997840172786177   0.5218665464382326     76.48275862068965                 [('move', 34), ('black', 34), ('white', 33), ('one', 27), ('king', 26)]\n",
      "Humanities/Social Science                   25          2654               769  0.4938985228002569   0.5866616428033158                106.16      [('arabic', 27), ('one', 19), ('hebrew', 19), ('provide', 19), ('answer', 18)]          3812              1341   0.5946784922394679   0.5915529905561385                152.48        [('arabic', 18), ('image', 17), ('hebrew', 17), ('knowledge', 15), ('text', 13)]\n",
      "Math                                        45          3893               800  0.4257583821181479   0.4826611867454405      86.5111111111111            [('answer', 32), ('circle', 22), ('agi', 19), ('square', 17), ('c', 16)]          6916              1101  0.35804878048780486  0.44462116830537884     153.6888888888889                     [('b', 48), ('r', 37), ('area', 33), ('triangle', 32), ('mid', 30)]\n",
      "Physics                                     14          1408               418  0.6174298375184638  0.48082386363636365    100.57142857142857  [('interference', 11), ('pattern', 11), ('point', 10), ('show', 9), ('result', 9)]          2572               619   0.5020275750202757  0.47939346811819594    183.71428571428572        [('r', 20), ('n', 18), ('beginequation', 17), ('endequation', 17), ('left', 13)]\n",
      "Computer Science/AI                         15          2617               503   0.518022657054583  0.37103553687428353    174.46666666666667           [('item', 27), ('image', 17), ('us', 15), ('canada', 15), ('mexico', 15)]          3140               765   0.4564439140811456   0.5337579617834395    209.33333333333334       [('local', 37), ('image', 20), ('variables', 20), ('code', 19), ('putexcel', 17)]\n",
      "Biology/Medicine                            59          6452              1411  0.4181979845880261    0.522938623682579    109.35593220338983                [('image', 57), ('b', 56), ('c', 48), ('right', 35), ('answer', 34)]          6479              1653  0.47094017094017093    0.541750270103411     109.8135593220339               [('image', 52), ('species', 25), ('c', 23), ('shows', 22), ('right', 21)]\n",
      "Chemistry                                   62          4409               797   0.355011135857461   0.5091857564073486     71.11290322580645                [('reaction', 51), ('b', 48), ('c', 44), ('product', 43), ('h', 36)]          6572              1357  0.39401858304297327    0.524041387705417                 106.0  [('reaction', 43), ('group', 39), ('compound', 38), ('one', 31), ('intermediate', 29)]\n",
      "Engineering                                 52          6221               981  0.2835260115606936   0.5561806783475326    119.63461538461539      [('power', 156), ('system', 79), ('voltage', 73), ('bus', 54), ('losses', 52)]         11467              1088  0.20355472404115996   0.4661201709252638    220.51923076923077      [('times', 303), ('power', 206), ('voltage', 124), ('textmw', 95), ('approx', 93)]\n",
      "\n",
      "==================================================\n",
      "\n",
      "Dataset Analysis Summary for Humanity's Last Exam (HLE):\n",
      "- Total Questions: 2700, Training: 1889 (70%), Test: 811 (30%)\n",
      "- Stratified Split: Proportional across 8 categories, 0% exact-match, and 12% image presence (Chi2 p-values mostly > 0.05)\n",
      "- Image Questions: 330 (12.22%), varying by category\n",
      "- Text Metrics: Avg 115.7 words/question, Avg 199.5 words/rationale, Avg vocab size 4622 (content words only)\n",
      "- Findings: HLE’s 10% image questions and 80% exact-match format, balanced across categories, challenge text-only LLMs (baseline <14%). Concise questions with detailed rationales and multimodal elements suggest fine-tuning LLaMA with GRPO must leverage both text and image data for reasoning gains.\n"
     ]
    }
   ],
   "source": [
    "# Updated complex_stratified_split with correct image handling\n",
    "def complex_stratified_split(df, train_size=0.7, stratify_cols=[\"category\", \"answer_type\", \"has_image\"], random_state=72):\n",
    "  # Define has_image: True if image is non-null and non-empty string\n",
    "  df[\"has_image\"] = df[\"image\"].apply(lambda x: isinstance(x, str) and len(x.strip()) > 0)\n",
    "\n",
    "  def create_strat_key(row):\n",
    "    return \"_\".join(str(row[col]) if pd.notnull(row[col]) else \"None\" for col in stratify_cols)\n",
    "\n",
    "  df[\"strat_key\"] = df.apply(create_strat_key, axis=1)\n",
    "  stratum_counts = df[\"strat_key\"].value_counts()\n",
    "  print(\"Stratum Counts:\\n\", stratum_counts)\n",
    "  sparse_strata = stratum_counts[stratum_counts < 2].index\n",
    "\n",
    "  if len(sparse_strata) > 0:\n",
    "    print(f\"Warning: {len(sparse_strata)} sparse strata. Falling back.\")\n",
    "    for col in stratify_cols[::-1]:\n",
    "      simplified_df = df.copy()\n",
    "      simplified_df[\"strat_key\"] = simplified_df[col].fillna(\"None\")\n",
    "      if simplified_df[\"strat_key\"].value_counts().min() >= 2:\n",
    "        print(f\"Fallback: Stratifying by '{col}' only.\")\n",
    "        train_df, test_df = train_test_split(df, train_size=train_size, stratify=simplified_df[\"strat_key\"], random_state=random_state)\n",
    "        train_df = train_df.drop(columns=[\"strat_key\", \"has_image\"])\n",
    "        test_df = test_df.drop(columns=[\"strat_key\", \"has_image\"])\n",
    "        return train_df, test_df\n",
    "    print(\"Using random split.\")\n",
    "    train_df, test_df = train_test_split(df, train_size=train_size, random_state=random_state)\n",
    "  else:\n",
    "    train_df, test_df = train_test_split(df, train_size=train_size, stratify=df[\"strat_key\"], random_state=random_state)\n",
    "\n",
    "  train_df = train_df.drop(columns=[\"strat_key\", \"has_image\"])\n",
    "  test_df = test_df.drop(columns=[\"strat_key\", \"has_image\"])\n",
    "  return train_df, test_df\n",
    "\n",
    "\n",
    "# Text metrics with stopword handling\n",
    "def compute_text_metrics(text_series):\n",
    "  stop_words = set(stopwords.words(\"english\"))\n",
    "  tokens = text_series.apply(lambda x: word_tokenize(re.sub(r\"[^\\w\\s]\", \"\", str(x).lower())) if pd.notnull(x) else [])\n",
    "  all_words = [word for tokens_list in tokens for word in tokens_list]\n",
    "  content_words = [word for word in all_words if word not in stop_words and word.isalpha()]\n",
    "\n",
    "  total_words = len(all_words)\n",
    "  vocab_size = len(set(content_words))\n",
    "  ttr = vocab_size / len(content_words) if content_words else 0\n",
    "  lexical_density = len(content_words) / total_words if total_words > 0 else 0\n",
    "  avg_words = total_words / len(text_series)\n",
    "  freq_dist = Counter(content_words).most_common(5)\n",
    "  truncated_freq_dist = [(word[:47] + \"...\" if len(word) > 50 else word, count) for word, count in freq_dist]\n",
    "\n",
    "  return {\n",
    "    \"Total Words\": total_words,\n",
    "    \"Vocabulary Size\": vocab_size,\n",
    "    \"TTR\": ttr,\n",
    "    \"Lexical Density\": lexical_density,\n",
    "    \"Avg Words per Entry\": avg_words,\n",
    "    \"Top 5 Words\": truncated_freq_dist,\n",
    "  }\n",
    "\n",
    "\n",
    "def validate_split(df, train_df, test_df, strat_col):\n",
    "  full_dist = df[strat_col].value_counts(normalize=True)\n",
    "  train_dist = train_df[strat_col].value_counts(normalize=True)\n",
    "  test_dist = test_df[strat_col].value_counts(normalize=True)\n",
    "  contingency = pd.concat([full_dist, train_dist, test_dist], axis=1).fillna(0)\n",
    "  contingency.columns = [\"Full\", \"Train\", \"Test\"]\n",
    "  chi2, p, _, _ = chi2_contingency(contingency * len(df))\n",
    "  return contingency, chi2, p\n",
    "\n",
    "\n",
    "# Perform split\n",
    "train_df, test_df = complex_stratified_split(df)\n",
    "\n",
    "# Category-wise analysis\n",
    "categories = df[\"category\"].unique()\n",
    "category_stats = {}\n",
    "for cat in categories:\n",
    "  cat_df = df[df[\"category\"] == cat]\n",
    "  cat_train_df = train_df[train_df[\"category\"] == cat]\n",
    "  cat_test_df = test_df[test_df[\"category\"] == cat]\n",
    "  question_metrics = compute_text_metrics(cat_df[\"question\"])\n",
    "  rationale_metrics = compute_text_metrics(cat_df[\"rationale\"])\n",
    "  image_prop = cat_df[\"image\"].apply(lambda x: isinstance(x, str) and len(x.strip()) > 0).mean()\n",
    "  answer_type_dist = cat_df[\"answer_type\"].value_counts(normalize=True)\n",
    "  contingency_image, chi2_image, p_image = validate_split(cat_df, cat_train_df, cat_test_df, \"image\")\n",
    "  contingency_answer, chi2_answer, p_answer = validate_split(cat_df, cat_train_df, cat_test_df, \"answer_type\")\n",
    "\n",
    "  category_stats[cat] = {\n",
    "    \"Question Metrics\": question_metrics,\n",
    "    \"Rationale Metrics\": rationale_metrics,\n",
    "    \"Image Proportion\": image_prop,\n",
    "    \"Answer Type Dist\": answer_type_dist.to_dict(),\n",
    "    \"Image Split Contingency\": contingency_image,\n",
    "    \"Image Chi2\": chi2_image,\n",
    "    \"Image P-value\": p_image,\n",
    "    \"Answer Type Split Contingency\": contingency_answer,\n",
    "    \"Answer Type Chi2\": chi2_answer,\n",
    "    \"Answer Type P-value\": p_answer,\n",
    "  }\n",
    "\n",
    "# Image cross-category analysis\n",
    "image_df = df[df[\"image\"].apply(lambda x: isinstance(x, str) and len(x.strip()) > 0)]\n",
    "image_cross_stats = {}\n",
    "for cat in categories:\n",
    "  cat_image_df = image_df[image_df[\"category\"] == cat]\n",
    "  question_metrics = compute_text_metrics(cat_image_df[\"question\"])\n",
    "  rationale_metrics = compute_text_metrics(cat_image_df[\"rationale\"])\n",
    "  image_cross_stats[cat] = {\"Num Image Questions\": len(cat_image_df), \"Question Metrics\": question_metrics, \"Rationale Metrics\": rationale_metrics}\n",
    "\n",
    "\n",
    "# Display with truncation\n",
    "def truncate_string(value, max_len=200):\n",
    "  if isinstance(value, str) and len(value) > max_len:\n",
    "    return value[: max_len - 3] + \"...\"\n",
    "  return value\n",
    "\n",
    "\n",
    "print(\"\\n=== Category-wise Analysis ===\")\n",
    "for cat, stats in category_stats.items():\n",
    "  print(f\"\\nCategory: {truncate_string(cat)}\")\n",
    "  print(\"\\nQuestion Text Metrics:\")\n",
    "  print(pd.DataFrame([{k: truncate_string(str(v)) for k, v in stats[\"Question Metrics\"].items()}]).T.to_string())\n",
    "  print(\"\\nRationale Text Metrics:\")\n",
    "  print(pd.DataFrame([{k: truncate_string(str(v)) for k, v in stats[\"Rationale Metrics\"].items()}]).T.to_string())\n",
    "  print(f\"\\nImage Proportion: {stats['Image Proportion']:.2%}\")\n",
    "  print(\"\\nAnswer Type Distribution:\")\n",
    "  print(pd.Series({k: truncate_string(str(v)) for k, v in stats[\"Answer Type Dist\"].items()}).to_string())\n",
    "  # print(\"\\n\\nImage Split Contingency Table:\")\n",
    "  # print(stats['Image Split Contingency'].to_string())\n",
    "  print(f\"Image Chi2: {stats['Image Chi2']:.2f}, P-value: {stats['Image P-value']:.4f}\")\n",
    "  print(\"\\nAnswer Type Split Contingency Table:\")\n",
    "  print(stats[\"Answer Type Split Contingency\"].to_string())\n",
    "  print(f\"Answer Type Chi2: {stats['Answer Type Chi2']:.2f}, P-value: {stats['Answer Type P-value']:.4f}\")\n",
    "\n",
    "print(\"\\n=== Image Cross-Category Analysis ===\")\n",
    "image_table = pd.DataFrame(\n",
    "  [\n",
    "    {\n",
    "      **{\"Category\": truncate_string(cat), \"Num Image Questions\": stats[\"Num Image Questions\"]},\n",
    "      **{f\"Q_{k}\": truncate_string(str(v)) for k, v in stats[\"Question Metrics\"].items()},\n",
    "      **{f\"R_{k}\": truncate_string(str(v)) for k, v in stats[\"Rationale Metrics\"].items()},\n",
    "    }\n",
    "    for cat, stats in image_cross_stats.items()\n",
    "  ]\n",
    ").set_index(\"Category\")\n",
    "print(image_table.to_string())\n",
    "\n",
    "# Summary\n",
    "total_questions = len(df)\n",
    "train_size = len(train_df)\n",
    "test_size = len(test_df)\n",
    "image_questions = df[\"image\"].apply(lambda x: isinstance(x, str) and len(x.strip()) > 0).sum()\n",
    "avg_words_question = compute_text_metrics(df[\"question\"])[\"Avg Words per Entry\"]\n",
    "avg_words_rationale = compute_text_metrics(df[\"rationale\"])[\"Avg Words per Entry\"]\n",
    "avg_vocab_size = np.mean([stats[\"Question Metrics\"][\"Vocabulary Size\"] for stats in category_stats.values()])\n",
    "image_dependency = image_questions / total_questions\n",
    "category_count = len(categories)\n",
    "exact_match_prop = df[\"answer_type\"].value_counts(normalize=True).get(\"exact-match\", 0)\n",
    "\n",
    "summary = (\n",
    "  f\"Dataset Analysis Summary for Humanity's Last Exam (HLE):\\n\"\n",
    "  f\"- Total Questions: {total_questions}, Training: {train_size} (70%), Test: {test_size} (30%)\\n\"\n",
    "  f\"- Stratified Split: Proportional across {category_count} categories, {exact_match_prop:.0%} exact-match, \"\n",
    "  f\"and {image_dependency:.0%} image presence (Chi2 p-values mostly > 0.05)\\n\"\n",
    "  f\"- Image Questions: {image_questions} ({image_dependency:.2%}), varying by category\\n\"\n",
    "  f\"- Text Metrics: Avg {avg_words_question:.1f} words/question, Avg {avg_words_rationale:.1f} words/rationale, \"\n",
    "  f\"Avg vocab size {avg_vocab_size:.0f} (content words only)\\n\"\n",
    "  f\"- Findings: HLE’s 10% image questions and 80% exact-match format, balanced across categories, \"\n",
    "  f\"challenge text-only LLMs (baseline <14%). Concise questions with detailed rationales and multimodal \"\n",
    "  f\"elements suggest fine-tuning LLaMA with GRPO must leverage both text and image data for reasoning gains.\"\n",
    ")\n",
    "print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Scale and Structure\n",
      "- Total Questions: 2700\n",
      "- Exact-Match Proportion: 0.00%\n",
      "- Multiple-Choice Proportion: 0.00%\n",
      "- Image Proportion: 12.22%\n",
      "- Category Distribution:\n",
      "  - Math: 40.96%\n",
      "  - Biology/Medicine: 11.22%\n",
      "  - Other: 9.56%\n",
      "  - Computer Science/AI: 9.56%\n",
      "  - Physics: 8.89%\n",
      "  - Humanities/Social Science: 8.70%\n",
      "  - Chemistry: 6.30%\n",
      "  - Engineering: 4.81%\n",
      "\n",
      "Textual Complexity\n",
      "- Average Words per Question: 183.61\n",
      "- Average Words per Rationale: 384.79\n",
      "- Vocabulary Size (Content Words): 25606\n",
      "- Lexical Density (Question): 31.00%\n",
      "- Lexical Density (Rationale): 24.99%\n",
      "- Top 5 Content Words per Category:\n",
      "  - Other: answer (221), black (175), white (169), one (163), x (161)\n",
      "  - Humanities/Social Science: answer (298), would (214), one (206), b (169), word (137)\n",
      "  - Math: x (15215), z (6865), n (4558), w (2627), b (2318)\n",
      "  - Physics: r (630), nm (417), wavelength (410), intensity (404), c (300)\n",
      "  - Computer Science/AI: x (907), n (778), k (446), b (387), f (387)\n",
      "  - Biology/Medicine: answer (408), patient (243), cells (240), protein (229), b (221)\n",
      "  - Chemistry: reaction (189), compound (140), product (139), group (122), n (117)\n",
      "  - Engineering: power (412), r (355), total (264), system (237), voltage (222)\n",
      "\n",
      "Reasoning Depth\n",
      "- Average Sentences per Rationale: 8.51\n",
      "- Average Reasoning Indicators per Rationale: 5.90\n",
      "- Average Math Notations per Rationale: 0.00\n",
      "\n",
      "Stratified Split Validation (Chi-Square p-values)\n",
      "- Category: 1.0000\n",
      "- Answer Type: 0.9965\n",
      "- Has Image: 0.9628\n"
     ]
    }
   ],
   "source": [
    "# Preprocess text: tokenize and remove stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "  \"\"\"Tokenize text and remove stopwords and punctuation.\"\"\"\n",
    "  tokens = word_tokenize(text.lower())\n",
    "  tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "  return tokens\n",
    "\n",
    "\n",
    "df[\"question_tokens\"] = df[\"question\"].apply(preprocess_text)\n",
    "df[\"rationale_tokens\"] = df[\"rationale\"].apply(preprocess_text)\n",
    "\n",
    "# **Dataset Scale and Structure Metrics**\n",
    "total_questions = len(df)\n",
    "exact_match_prop = (df[\"answer_type\"] == \"exact-match\").mean()\n",
    "multiple_choice_prop = (df[\"answer_type\"] == \"multiple-choice\").mean()\n",
    "df[\"has_image\"] = df[\"image\"].apply(lambda x: isinstance(x, str) and len(x.strip()) > 0)\n",
    "image_prop = df[\"has_image\"].mean()\n",
    "category_dist = df[\"category\"].value_counts(normalize=True)\n",
    "\n",
    "# **Textual Complexity Metrics**\n",
    "avg_words_question = df[\"question\"].apply(lambda x: len(word_tokenize(x))).mean()\n",
    "avg_words_rationale = df[\"rationale\"].apply(lambda x: len(word_tokenize(x))).mean()\n",
    "\n",
    "all_tokens = [token for sublist in df[\"question_tokens\"] + df[\"rationale_tokens\"] for token in sublist]\n",
    "vocab_size = len(set(all_tokens))\n",
    "\n",
    "total_words_question = sum(len(word_tokenize(text)) for text in df[\"question\"])\n",
    "total_words_rationale = sum(len(word_tokenize(text)) for text in df[\"rationale\"])\n",
    "content_words_question = sum(len(tokens) for tokens in df[\"question_tokens\"])\n",
    "content_words_rationale = sum(len(tokens) for tokens in df[\"rationale_tokens\"])\n",
    "lexical_density_question = content_words_question / total_words_question if total_words_question > 0 else 0\n",
    "lexical_density_rationale = content_words_rationale / total_words_rationale if total_words_rationale > 0 else 0\n",
    "\n",
    "top_words_per_category = {}\n",
    "for category in df[\"category\"].unique():\n",
    "  category_df = df[df[\"category\"] == category]\n",
    "  category_tokens = [token for sublist in category_df[\"question_tokens\"] + category_df[\"rationale_tokens\"] for token in sublist]\n",
    "  top_words = Counter(category_tokens).most_common(5)\n",
    "  top_words_per_category[category] = top_words\n",
    "\n",
    "# **Reasoning Depth Metrics**\n",
    "df[\"rationale_sentences\"] = df[\"rationale\"].apply(lambda x: len(sent_tokenize(x)))\n",
    "avg_sentences_rationale = df[\"rationale_sentences\"].mean()\n",
    "\n",
    "reasoning_indicators = [\"therefore\", \"because\", \"if\", \"then\", \"hence\", \"thus\", \"consequently\", \"since\", \"so\", \"implies\"]\n",
    "math_notations = [r\"\\\\frac\", r\"\\\\int\", r\"\\\\sum\", r\"\\\\prod\", r\"\\\\lim\", r\"\\\\sqrt\", r\"\\\\log\", r\"\\\\sin\", r\"\\\\cos\", r\"\\\\tan\"]\n",
    "\n",
    "\n",
    "def count_indicators(text, indicators):\n",
    "  \"\"\"Count occurrences of specific indicators in text.\"\"\"\n",
    "  return sum(text.lower().count(ind) for ind in indicators)\n",
    "\n",
    "\n",
    "df[\"reasoning_indicators\"] = df[\"rationale\"].apply(lambda x: count_indicators(x, reasoning_indicators))\n",
    "df[\"math_notations\"] = df[\"rationale\"].apply(lambda x: count_indicators(x, math_notations))\n",
    "avg_reasoning_indicators = df[\"reasoning_indicators\"].mean()\n",
    "avg_math_notations = df[\"math_notations\"].mean()\n",
    "\n",
    "\n",
    "# **Stratified Split Validation**\n",
    "def complex_stratified_split(df, test_size=0.2, random_state=72):\n",
    "  \"\"\"Perform a stratified split based on category, answer_type, and has_image.\"\"\"\n",
    "  stratify_cols = [\"category\", \"answer_type\", \"has_image\"]\n",
    "  df[\"stratify_key\"] = df[stratify_cols].apply(lambda x: \"_\".join(x.astype(str)), axis=1)\n",
    "  train_df, test_df = train_test_split(df, test_size=test_size, stratify=df[\"stratify_key\"], random_state=random_state)\n",
    "  return train_df.drop(columns=\"stratify_key\"), test_df.drop(columns=\"stratify_key\")\n",
    "\n",
    "\n",
    "def validate_split(df, train_df, test_df, strat_col):\n",
    "  \"\"\"Validate stratification using chi-square test.\"\"\"\n",
    "  full_dist = df[strat_col].value_counts(normalize=True)\n",
    "  train_dist = train_df[strat_col].value_counts(normalize=True)\n",
    "  test_dist = test_df[strat_col].value_counts(normalize=True)\n",
    "  contingency = pd.concat([full_dist, train_dist, test_dist], axis=1).fillna(0)\n",
    "  contingency.columns = [\"Full\", \"Train\", \"Test\"]\n",
    "  chi2, p, _, _ = chi2_contingency(contingency * len(df))\n",
    "  return p\n",
    "\n",
    "\n",
    "train_df, test_df = complex_stratified_split(df)\n",
    "category_p = validate_split(df, train_df, test_df, \"category\")\n",
    "answer_type_p = validate_split(df, train_df, test_df, \"answer_type\")\n",
    "has_image_p = validate_split(df, train_df, test_df, \"has_image\")\n",
    "\n",
    "# **Output Results**\n",
    "print(\"Dataset Scale and Structure\")\n",
    "print(f\"- Total Questions: {total_questions}\")\n",
    "print(f\"- Exact-Match Proportion: {exact_match_prop:.2%}\")\n",
    "print(f\"- Multiple-Choice Proportion: {multiple_choice_prop:.2%}\")\n",
    "print(f\"- Image Proportion: {image_prop:.2%}\")\n",
    "print(\"- Category Distribution:\")\n",
    "for category, prop in category_dist.items():\n",
    "  print(f\"  - {category}: {prop:.2%}\")\n",
    "\n",
    "print(\"\\nTextual Complexity\")\n",
    "print(f\"- Average Words per Question: {avg_words_question:.2f}\")\n",
    "print(f\"- Average Words per Rationale: {avg_words_rationale:.2f}\")\n",
    "print(f\"- Vocabulary Size (Content Words): {vocab_size}\")\n",
    "print(f\"- Lexical Density (Question): {lexical_density_question:.2%}\")\n",
    "print(f\"- Lexical Density (Rationale): {lexical_density_rationale:.2%}\")\n",
    "print(\"- Top 5 Content Words per Category:\")\n",
    "for category, words in top_words_per_category.items():\n",
    "  print(f\"  - {category}: {', '.join([f'{word} ({count})' for word, count in words])}\")\n",
    "\n",
    "print(\"\\nReasoning Depth\")\n",
    "print(f\"- Average Sentences per Rationale: {avg_sentences_rationale:.2f}\")\n",
    "print(f\"- Average Reasoning Indicators per Rationale: {avg_reasoning_indicators:.2f}\")\n",
    "print(f\"- Average Math Notations per Rationale: {avg_math_notations:.2f}\")\n",
    "\n",
    "print(\"\\nStratified Split Validation (Chi-Square p-values)\")\n",
    "print(f\"- Category: {category_p:.4f}\")\n",
    "print(f\"- Answer Type: {answer_type_p:.4f}\")\n",
    "print(f\"- Has Image: {has_image_p:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n",
      "Dataset columns: ['id', 'question', 'image', 'image_preview', 'answer', 'answer_type', 'author_name', 'rationale', 'rationale_image', 'raw_subject', 'category', 'canary']\n",
      "\n",
      "Token and Image Statistics by Category (Plain Text for Inspection):\n",
      "                 Category                                Text Tokens            Image Size With Images\n",
      "         Biology/Medicine  min=11 - max=3008 (mean=257.13 ±sd344.08) mean=0.3582 ±sd0.3318     19.47\\%\n",
      "                Chemistry  min=16 - max=1699 (mean=186.07 ±sd259.06) mean=0.0655 ±sd0.0967     36.47\\%\n",
      "      Computer Science/AI  min=15 - max=5052 (mean=420.34 ±sd648.82) mean=0.2335 ±sd0.3776      5.81\\%\n",
      "              Engineering  min=14 - max=8972 (mean=396.45 ±sd919.56) mean=0.1733 ±sd0.2037      40.0\\%\n",
      "Humanities/Social Science  min=15 - max=1294 (mean=201.03 ±sd227.64) mean=0.3205 ±sd0.3624     10.64\\%\n",
      "                     Math min=15 - max=13518 (mean=216.81 ±sd483.85)   mean=0.145 ±sd0.211      4.07\\%\n",
      "                    Other  min=11 - max=7156 (mean=174.16 ±sd478.07) mean=0.3883 ±sd0.4474     22.48\\%\n",
      "                  Physics   min=11 - max=7313 (mean=248.2 ±sd520.54)  mean=0.1423 ±sd0.253      5.83\\%\n",
      "\n",
      "LaTeX table saved to '../.assets/dataset_description.tex'\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer for Llama-3.2-1B-Instruct\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "\n",
    "# Load the dataset from Hugging Face (cais/hle, test split)\n",
    "print(\"Dataset loaded successfully.\")\n",
    "\n",
    "# Check dataset columns\n",
    "print(\"Dataset columns:\", dataset.column_names)\n",
    "\n",
    "# Ensure required columns exist\n",
    "required_columns = {\"question\", \"answer\", \"image\", \"category\"}\n",
    "if not required_columns.issubset(dataset.column_names):\n",
    "  print(f\"Warning: Dataset missing some required columns {required_columns}. Adjust column names in script.\")\n",
    "  exit()\n",
    "\n",
    "\n",
    "def analyze_tokens_and_images(example):\n",
    "  \"\"\"\n",
    "    Analyze token counts for 'question' and 'answer', and quantify base64 image data.\n",
    "    \"\"\"\n",
    "  # Tokenize question and count tokens\n",
    "  question_text = example.get(\"question\", \"\")\n",
    "  question_tokens = tokenizer(question_text, truncation=False, padding=False, return_tensors=\"pt\")\n",
    "  question_token_count = question_tokens[\"input_ids\"].shape[1] if question_text else 0\n",
    "\n",
    "  # Tokenize answer and count tokens\n",
    "  answer_text = example.get(\"answer\", \"\")\n",
    "  answer_tokens = tokenizer(answer_text, truncation=False, padding=False, return_tensors=\"pt\")\n",
    "  answer_token_count = answer_tokens[\"input_ids\"].shape[1] if answer_text else 0\n",
    "\n",
    "  # Handle base64 image with potential data URI prefix\n",
    "  image_data = example.get(\"image\", \"\")\n",
    "  image_size = 0\n",
    "  if image_data:\n",
    "    # Strip data URI prefix (e.g., \"data:image/png;base64,\")\n",
    "    base64_string = re.sub(r\"^data:image/[^;]+;base64,\", \"\", image_data)\n",
    "    try:\n",
    "      decoded_image = base64.b64decode(base64_string)\n",
    "      image_size = len(decoded_image)  # Byte length\n",
    "    except Exception as e:\n",
    "      print(f\"Error decoding image for ID {example.get('id', 'unknown')}: {e}\")\n",
    "      image_size = 0\n",
    "\n",
    "  return {\"text_token_count\": question_token_count + answer_token_count, \"image_size_bytes\": image_size}  # Merge Q and A tokens\n",
    "\n",
    "\n",
    "# Apply the analysis function to the dataset\n",
    "analyzed_dataset = dataset.map(analyze_tokens_and_images)\n",
    "\n",
    "# Convert to pandas DataFrame for analysis\n",
    "analyzed_df = analyzed_dataset.to_pandas()\n",
    "\n",
    "# Group by category and calculate statistics\n",
    "grouped = analyzed_df.groupby(\"category\")\n",
    "\n",
    "# Initialize lists to build the table\n",
    "table_data = {\n",
    "  \"Category\": [],\n",
    "  \"With Images\": [],\n",
    "  \"Text Tokens Min\": [],\n",
    "  \"Text Tokens Max\": [],\n",
    "  \"Text Tokens Mean\": [],\n",
    "  \"Text Tokens SD\": [],\n",
    "  \"Img Size Mean\": [],\n",
    "  \"Img Size SD\": [],\n",
    "}\n",
    "\n",
    "# Iterate over each category\n",
    "for category, group in grouped:\n",
    "  # Percentage of examples with images (image_size_bytes > 0)\n",
    "  pct_with_images = (group[\"image_size_bytes\"] > 0).mean() * 100\n",
    "\n",
    "  # Filter group for image stats (only rows with valid images)\n",
    "  image_group = group[group[\"image_size_bytes\"] > 0]\n",
    "\n",
    "  # Append stats to table_data\n",
    "  table_data[\"Category\"].append(category)\n",
    "  table_data[\"With Images\"].append(pct_with_images)\n",
    "\n",
    "  # Text token stats (merged question + answer)\n",
    "  table_data[\"Text Tokens Min\"].append(group[\"text_token_count\"].min())\n",
    "  table_data[\"Text Tokens Max\"].append(group[\"text_token_count\"].max())\n",
    "  table_data[\"Text Tokens Mean\"].append(group[\"text_token_count\"].mean())\n",
    "  table_data[\"Text Tokens SD\"].append(group[\"text_token_count\"].std())\n",
    "\n",
    "  # Image size stats in MB (only for valid images; use NaN if no images)\n",
    "  if len(image_group) > 0:\n",
    "    table_data[\"Img Size Mean\"].append(image_group[\"image_size_bytes\"].mean() / 1048576)  # Bytes to MB\n",
    "    table_data[\"Img Size SD\"].append(image_group[\"image_size_bytes\"].std() / 1048576)\n",
    "  else:\n",
    "    table_data[\"Img Size Mean\"].append(np.nan)\n",
    "    table_data[\"Img Size SD\"].append(np.nan)\n",
    "\n",
    "# Create DataFrame for the table\n",
    "table_df = pd.DataFrame(table_data)\n",
    "\n",
    "# Format the table for display (plain text version)\n",
    "table_df[\"With Images\"] = table_df[\"With Images\"].round(2).astype(str) + \"\\\\%\"\n",
    "table_df[\"Text Tokens Mean\"] = table_df[\"Text Tokens Mean\"].round(2)\n",
    "table_df[\"Text Tokens SD\"] = table_df[\"Text Tokens SD\"].round(2)\n",
    "table_df[\"Img Size Mean\"] = table_df[\"Img Size Mean\"].round(4)\n",
    "table_df[\"Img Size SD\"] = table_df[\"Img Size SD\"].round(4)\n",
    "\n",
    "# Plain text version with characters for inspection\n",
    "table_df_plain = table_df.copy()\n",
    "table_df_plain[\"Text Tokens\"] = table_df_plain.apply(\n",
    "  lambda row: f\"min={int(row['Text Tokens Min'])} - max={int(row['Text Tokens Max'])} (mean={row['Text Tokens Mean']} ±sd{row['Text Tokens SD']})\", axis=1\n",
    ")\n",
    "table_df_plain[\"Image Size\"] = table_df_plain.apply(\n",
    "  lambda row: f\"mean={row['Img Size Mean']} ±sd{row['Img Size SD']}\" if not np.isnan(row[\"Img Size Mean\"]) else \"N/A\", axis=1\n",
    ")\n",
    "# Move \"With Images\" to the end for plain text\n",
    "table_df_plain = table_df_plain[[\"Category\", \"Text Tokens\", \"Image Size\", \"With Images\"]].sort_values(\"Category\")\n",
    "\n",
    "# LaTeX version with Greek symbols\n",
    "table_df[\"Text Tokens\"] = table_df.apply(\n",
    "  lambda row: f\"$\\\\downarrow${int(row['Text Tokens Min'])} - $\\\\uparrow${int(row['Text Tokens Max'])} {row['Text Tokens Mean']}$\\\\mu$ $\\\\pm${row['Text Tokens SD']}$\\\\sigma$\",\n",
    "  axis=1,\n",
    ")\n",
    "table_df[\"Image Size\"] = table_df.apply(\n",
    "  lambda row: f\"{row['Img Size Mean']}$\\\\mu$ $\\\\pm${row['Img Size SD']}$\\\\sigma$\" if not np.isnan(row[\"Img Size Mean\"]) else \"N/A\", axis=1\n",
    ")\n",
    "# Move \"With Images\" to the end for LaTeX\n",
    "table_df = table_df[[\"Category\", \"Text Tokens\", \"Image Size\", \"With Images\"]].sort_values(\"Category\")\n",
    "\n",
    "# Display plain text version\n",
    "print(\"\\nToken and Image Statistics by Category (Plain Text for Inspection):\")\n",
    "print(table_df_plain.to_string(index=False))\n",
    "\n",
    "# Convert to LaTeX and save to ../.assets/dataset_description.tex\n",
    "latex_table = table_df.to_latex(index=False, column_format=\"llll\", header=[\"Category\", \"Text Tokens \", \"Image Size (MB)\", \"Images\"], escape=False)\n",
    "\n",
    "# Ensure .assets directory exists\n",
    "os.makedirs(\".assets\", exist_ok=True)\n",
    "\n",
    "# Write LaTeX table with legend to file\n",
    "with open(\"../.assets/dataset_description.tex\", \"w\") as f:\n",
    "  f.write(\"\\\\begin{table}[H]\\n\")\n",
    "  f.write(\"\\\\centering\\n\")\n",
    "  f.write(latex_table)\n",
    "  f.write(\"\\\\vspace{0.2cm}\\n\")\n",
    "  f.write(\"\\\\caption{Dataset Description by Category for cais/hle Test Split}\\n\")\n",
    "  f.write(\"\\\\label{tab:dataset_description}\\n\")\n",
    "  f.write(\"\\\\end{table}\\n\")\n",
    "\n",
    "print(\"\\nLaTeX table saved to '../.assets/dataset_description.tex'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stratification Statistics by Category (Plain Text):\n",
      "                 Category  Questions Train % Test % % Image % Exact Match\n",
      "         Biology/Medicine        303  52.81% 47.19%  19.58%        48.25%\n",
      "                Chemistry        170  38.24% 61.76%  38.10%        75.24%\n",
      "      Computer Science/AI        258  48.84% 51.16%   6.82%        69.70%\n",
      "              Engineering        130  32.31% 67.69%  46.59%        72.73%\n",
      "Humanities/Social Science        235  46.38% 53.62%  11.11%        59.52%\n",
      "                     Math       1106  80.29% 19.71%   4.13%        91.28%\n",
      "                    Other        258  48.84% 51.16%  22.73%        75.00%\n",
      "                  Physics        240  47.08% 52.92%   6.30%        84.25%\n",
      "\n",
      "LaTeX table saved to '../.assets/split_statistics.tex'\n"
     ]
    }
   ],
   "source": [
    "# Custom split function with while loop, exact balancing, no sampling\n",
    "def split_dataset(df, min_test_pcts, random_state=72):\n",
    "  df[\"has_image\"] = df[\"image\"].apply(lambda x: isinstance(x, str) and len(x.strip()) > 0 and \"data:image\" in x)\n",
    "\n",
    "  train_dfs = []\n",
    "  test_dfs = []\n",
    "  for cat in df[\"category\"].unique():\n",
    "    cat_df = df[df[\"category\"] == cat].copy()\n",
    "    total_size = len(cat_df)\n",
    "    min_test_pct = min_test_pcts.get(cat, 50.0)  # Default to 50% if category not in min_test_pcts\n",
    "    test_size = int(round(total_size * (min_test_pct / 100)))\n",
    "    train_size = total_size - test_size\n",
    "\n",
    "    if len(cat_df) < 2:\n",
    "      print(f\"Warning: Category '{cat}' has too few samples ({len(cat_df)}). Using basic split.\")\n",
    "      train_cat = cat_df.iloc[:train_size]\n",
    "      test_cat = cat_df.iloc[train_size:]\n",
    "    else:\n",
    "      # Calculate target percentages from full category\n",
    "      target_image_pct = cat_df[\"has_image\"].mean() * 100\n",
    "      target_exact_pct = (cat_df[\"answer_type\"] == \"exactMatch\").mean() * 100\n",
    "\n",
    "      # Initialize test set\n",
    "      test_cat = pd.DataFrame(columns=cat_df.columns)\n",
    "      available = cat_df.copy()\n",
    "\n",
    "      # Fill test set with while loop until criteria met\n",
    "      while len(test_cat) < test_size:\n",
    "        current_image_pct = (test_cat[\"has_image\"].mean() * 100) if len(test_cat) > 0 else 0\n",
    "        current_exact_pct = (test_cat[\"answer_type\"] == \"exactMatch\").mean() * 100 if len(test_cat) > 0 else 0\n",
    "\n",
    "        # Check if criteria are met within 1% tolerance\n",
    "        if (\n",
    "          len(test_cat) > 0\n",
    "          and abs(current_image_pct - target_image_pct) < 1.0\n",
    "          and abs(current_exact_pct - target_exact_pct) < 1.0\n",
    "          and len(test_cat) >= test_size\n",
    "        ):\n",
    "          break\n",
    "\n",
    "        # Select next row to balance\n",
    "        if current_image_pct < target_image_pct and len(available[available[\"has_image\"]]) > 0:\n",
    "          next_row = available[available[\"has_image\"]].iloc[0:1]\n",
    "        elif current_exact_pct < target_exact_pct and len(available[available[\"answer_type\"] == \"exactMatch\"]) > 0:\n",
    "          next_row = available[available[\"answer_type\"] == \"exactMatch\"].iloc[0:1]\n",
    "        elif len(available) > 0:\n",
    "          next_row = available.iloc[0:1]\n",
    "        else:\n",
    "          break\n",
    "\n",
    "        test_cat = pd.concat([test_cat, next_row])\n",
    "        available = available[~available.index.isin(next_row.index)]\n",
    "\n",
    "      # Trim or pad test set to exact size\n",
    "      if len(test_cat) > test_size:\n",
    "        test_cat = test_cat.iloc[:test_size]\n",
    "      elif len(test_cat) < test_size:\n",
    "        extra_needed = test_size - len(test_cat)\n",
    "        extra_rows = available.iloc[:extra_needed]\n",
    "        test_cat = pd.concat([test_cat, extra_rows])\n",
    "        available = available[~available.index.isin(extra_rows.index)]\n",
    "\n",
    "      # Remaining rows go to train\n",
    "      train_cat = available\n",
    "\n",
    "    train_dfs.append(train_cat)\n",
    "    test_dfs.append(test_cat)\n",
    "\n",
    "  train_df = pd.concat(train_dfs).drop(columns=[\"has_image\"])\n",
    "  test_df = pd.concat(test_dfs).drop(columns=[\"has_image\"])\n",
    "  return train_df, test_df\n",
    "\n",
    "\n",
    "# Minimum test percentages from your table (Feb 20, 2025, 10:58 PM)\n",
    "min_test_pcts = {\n",
    "  \"Biology/Medicine\": 47.26,\n",
    "  \"Chemistry\": 61.56,\n",
    "  \"Computer Science/AI\": 51.29,\n",
    "  \"Engineering\": 67.72,\n",
    "  \"Humanities/Social Science\": 53.63,\n",
    "  \"Math\": 19.67,\n",
    "  \"Other\": 51.29,\n",
    "  \"Physics\": 53.10,\n",
    "}\n",
    "\n",
    "# Perform the split\n",
    "train_df, test_df = split_dataset(df, min_test_pcts)\n",
    "\n",
    "# Calculate stats by category with percentages\n",
    "categories = df[\"category\"].unique()\n",
    "strat_stats = {}\n",
    "for cat in categories:\n",
    "  cat_df = df[df[\"category\"] == cat]\n",
    "  cat_train_df = train_df[train_df[\"category\"] == cat]\n",
    "  cat_test_df = test_df[test_df[\"category\"] == cat]\n",
    "\n",
    "  num_samples = len(cat_df)\n",
    "  test_size = len(cat_test_df)\n",
    "  train_size = num_samples - test_size\n",
    "  train_pct = (train_size / num_samples) * 100 if num_samples > 0 else 0\n",
    "  test_pct = (test_size / num_samples) * 100 if num_samples > 0 else 0\n",
    "\n",
    "  test_pct_image = (\n",
    "    cat_test_df[\"image\"].apply(lambda x: isinstance(x, str) and len(x.strip()) > 0 and \"data:image\" in x).mean() * 100 if len(cat_test_df) > 0 else 0\n",
    "  )\n",
    "\n",
    "  test_pct_exact = (cat_test_df[\"answer_type\"] == \"exactMatch\").mean() * 100 if len(cat_test_df) > 0 else 0\n",
    "\n",
    "  strat_stats[cat] = {\"Questions\": num_samples, \"Train %\": train_pct, \"Test %\": test_pct, \"% Image\": test_pct_image, \"% Exact Match\": test_pct_exact}\n",
    "\n",
    "# Create table DataFrame\n",
    "table_data = {\"Category\": [], \"Questions\": [], \"Train %\": [], \"Test %\": [], \"% Image\": [], \"% Exact Match\": []}\n",
    "for cat, stats in strat_stats.items():\n",
    "  table_data[\"Category\"].append(cat)\n",
    "  table_data[\"Questions\"].append(stats[\"Questions\"])\n",
    "  table_data[\"Train %\"].append(stats[\"Train %\"])\n",
    "  table_data[\"Test %\"].append(stats[\"Test %\"])\n",
    "  table_data[\"% Image\"].append(stats[\"% Image\"])\n",
    "  table_data[\"% Exact Match\"].append(stats[\"% Exact Match\"])\n",
    "\n",
    "table_df = pd.DataFrame(table_data)\n",
    "table_df = table_df.sort_values(\"Category\")\n",
    "\n",
    "# Format plain text version\n",
    "table_df_plain = table_df.copy()\n",
    "for col in [\"Train %\", \"Test %\", \"% Image\", \"% Exact Match\"]:\n",
    "  table_df_plain[col] = table_df_plain[col].apply(lambda x: f\"{x:.2f}%\")\n",
    "\n",
    "# Display plain text version\n",
    "print(\"\\nStratification Statistics by Category (Plain Text):\")\n",
    "print(table_df_plain.to_string(index=False))\n",
    "\n",
    "# Save LaTeX version with corrected path\n",
    "os.makedirs(\"../.assets\", exist_ok=True)\n",
    "latex_table = table_df.to_latex(\n",
    "  index=False,\n",
    "  column_format=\"lrrrrr\",\n",
    "  header=[\"Category\", \"Questions\", \"Train \\\\%\", \"Test \\\\%\", \"\\\\% Image\", \"\\\\% Exact Match\"],\n",
    "  escape=False,\n",
    "  float_format=\"%.2f\",\n",
    ")\n",
    "with open(\"../.assets/split_statistics.tex\", \"w\") as f:\n",
    "  f.write(\"\\\\begin{table}[H]\\n\")\n",
    "  f.write(\"\\\\centering\\n\")\n",
    "  f.write(latex_table)\n",
    "  f.write(\"\\\\vspace{0.2cm}\\n\")\n",
    "  f.write(\"\\\\caption{Dataset Train/Test Split by Category}\\n\")\n",
    "  f.write(\"\\\\label{tab:split_statistics}\\n\")\n",
    "  f.write(\"\\\\end{table}\\n\")\n",
    "\n",
    "print(\"\\nLaTeX table saved to '../.assets/split_statistics.tex'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stratification Statistics by Category (Plain Text):\n",
      "                 Category  Questions  Min Test Size Min Test % Train %\n",
      "         Biology/Medicine        303            143     47.26%  52.74%\n",
      "                Chemistry        170            105     61.56%  38.44%\n",
      "      Computer Science/AI        258            132     51.29%  48.71%\n",
      "              Engineering        130             88     67.72%  32.28%\n",
      "Humanities/Social Science        235            126     53.63%  46.37%\n",
      "                     Math       1106            218     19.67%  80.33%\n",
      "                    Other        258            132     51.29%  48.71%\n",
      "                  Physics        240            127      53.1%   46.9%\n",
      "\n",
      "LaTeX table saved to '../.assets/split_analysis.tex'\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate minimum test size and percentage\n",
    "def calculate_min_test_size(total_examples):\n",
    "  Z = 1.645  # 90% confidence level\n",
    "  p = 0.5  # Assumed proportion (max variance for conservative estimate)\n",
    "  E = 0.05  # 5% margin of error\n",
    "  n = (Z**2 * p * (1 - p)) / (E**2)  # Base sample size\n",
    "  n_adjusted = n / (1 + (n - 1) / total_examples) if total_examples > 0 else n  # Finite population correction\n",
    "  min_test_pct = (n_adjusted / total_examples) * 100 if total_examples > 0 else 100  # Convert to percentage\n",
    "  return n_adjusted, min_test_pct\n",
    "\n",
    "\n",
    "# Stratification stats by category\n",
    "categories = df[\"category\"].unique()\n",
    "strat_stats = {}\n",
    "for cat in categories:\n",
    "  cat_df = df[df[\"category\"] == cat]\n",
    "  total_examples = len(cat_df)\n",
    "  min_test_size, min_test_pct = calculate_min_test_size(total_examples)\n",
    "\n",
    "  strat_stats[cat] = {\"Questions\": total_examples, \"Min Test Size\": min_test_size, \"Min Test %\": min_test_pct, \"Train %\": 100 - min_test_pct}\n",
    "\n",
    "# Create table DataFrame\n",
    "table_data = {\"Category\": [], \"Questions\": [], \"Min Test Size\": [], \"Min Test %\": [], \"Train %\": []}\n",
    "for cat, stats in strat_stats.items():\n",
    "  table_data[\"Category\"].append(cat)\n",
    "  table_data[\"Questions\"].append(stats[\"Questions\"])\n",
    "  table_data[\"Min Test Size\"].append(stats[\"Min Test Size\"])\n",
    "  table_data[\"Min Test %\"].append(stats[\"Min Test %\"])\n",
    "  table_data[\"Train %\"].append(stats[\"Train %\"])\n",
    "\n",
    "table_df = pd.DataFrame(table_data)\n",
    "\n",
    "# Format plain text version\n",
    "table_df_plain = table_df.copy()\n",
    "table_df_plain[\"Min Test Size\"] = table_df_plain[\"Min Test Size\"].round(0).astype(int)\n",
    "table_df_plain[\"Min Test %\"] = table_df_plain[\"Min Test %\"].round(2).astype(str) + \"%\"\n",
    "table_df_plain[\"Train %\"] = table_df_plain[\"Train %\"].round(2).astype(str) + \"%\"\n",
    "table_df_plain = table_df_plain.sort_values(\"Category\")\n",
    "\n",
    "# Format LaTeX version\n",
    "table_df_latex = table_df.copy()\n",
    "table_df_latex[\"Min Test Size\"] = table_df_latex[\"Min Test Size\"].round(0).astype(int)\n",
    "table_df_latex[\"Min Test %\"] = table_df_latex[\"Min Test %\"].round(2)\n",
    "table_df_latex[\"Train %\"] = table_df_latex[\"Train %\"].round(2)\n",
    "table_df_latex = table_df_latex.sort_values(\"Category\")\n",
    "\n",
    "# Display plain text version\n",
    "print(\"\\nStratification Statistics by Category (Plain Text):\")\n",
    "print(table_df_plain.to_string(index=False))\n",
    "\n",
    "# Save LaTeX version\n",
    "os.makedirs(\"../.assets\", exist_ok=True)\n",
    "latex_table = table_df_latex.to_latex(\n",
    "  index=False, column_format=\"lrrrr\", header=[\"Category\", \"Questions\", \"Min Test Size\", \"Min Test \\\\%\", \"Train \\\\%\"], escape=False\n",
    ")\n",
    "with open(\"../.assets/split_analysis.tex\", \"w\") as f:\n",
    "  f.write(\"\\\\begin{table}[H]\\n\")\n",
    "  f.write(\"\\\\centering\\n\")\n",
    "  f.write(latex_table)\n",
    "  f.write(\"\\\\vspace{0.2cm}\\n\")\n",
    "  f.write(\"\\\\caption{Statistics by Category for cais/hle Test Split (90\\\\% CI, 5\\\\% Error)}\\n\")\n",
    "  f.write(\"\\\\label{tab:split_analysis}\\n\")\n",
    "  f.write(\"\\\\end{table}\\n\")\n",
    "\n",
    "print(\"\\nLaTeX table saved to '../.assets/split_analysis.tex'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
